{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10096,"status":"ok","timestamp":1652112253239,"user":{"displayName":"george mouts","userId":"12301814581979843830"},"user_tz":-180},"id":"GzafyTmK3yQQ","outputId":"7c690770-4db5-47ae-a34e-e67157439175"},"outputs":[{"output_type":"stream","name":"stderr","text":["fatal: destination path 'kaggle-environments' already exists and is not an empty directory.\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n","fatal: destination path 'football' already exists and is not an empty directory.\n"]}],"source":["%%bash\n","git clone -q https://github.com/Kaggle/kaggle-environments.git\n","cd kaggle-environments && pip3 install -qq . && cd ..\n","\n","apt-get -qq update\n","apt-get -qq install libsdl2-gfx-dev libsdl2-ttf-dev > /dev/null\n","\n","GRF_VER=v2.8\n","GRF_PATH=football/third_party/gfootball_engine/lib\n","GRF_URL=https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_${GRF_VER}.so\n","git clone -q -b ${GRF_VER} https://github.com/google-research/football.git\n","mkdir -p ${GRF_PATH}\n","wget -q ${GRF_URL} -O ${GRF_PATH}/prebuilt_gameplayfootball.so"]},{"cell_type":"code","source":[""],"metadata":{"id":"FHOS5oeEdRo0","executionInfo":{"status":"ok","timestamp":1652111646295,"user_tz":-180,"elapsed":29,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b_qvwQbE32B_","executionInfo":{"status":"ok","timestamp":1652112264536,"user_tz":-180,"elapsed":1309,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["#from gfootball.env.football_env import FootballEnv\n","from kaggle_environments import make\n","from gfootball.env.config import Config\n","import gfootball.env as football_env\n","\n","#import dqn libraries\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim \n","import numpy as np\n","import pandas as pd\n","import itertools\n","import utils\n","import random\n","from collections import deque\n","import matplotlib.pyplot as plt\n","#from utils import plot_learning_curve\n","#import env \n","import gym\n","import gfootball \n","import math\n","\n","#env_name = \"GFootballBase-v0\"\n","#print(env_name)"]},{"cell_type":"code","source":["%%writefile football/gfootball/scenarios/my_academy.py\n","from . import *\n","\n","\n","def build_scenario(builder):\n","  builder.config().game_duration = 400\n","  builder.config().deterministic = False\n","  builder.config().offsides = False\n","  builder.config().end_episode_on_score = True\n","  builder.config().end_episode_on_out_of_play = True\n","  builder.config().end_episode_on_possession_change = True\n","  builder.SetBallPosition(0.02, 0.0)\n","\n","  builder.SetTeam(Team.e_Left)\n","  builder.AddPlayer(-1.0, 0.0, e_PlayerRole_GK)\n","  builder.AddPlayer(0.0, 0.0, e_PlayerRole_CB)\n","\n","  builder.SetTeam(Team.e_Right)\n","  builder.AddPlayer(-1.0, 0.0, e_PlayerRole_GK)\n","  print(\"Inside scenario build\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qa6Kq7IokXRU","executionInfo":{"status":"ok","timestamp":1652112374226,"user_tz":-180,"elapsed":360,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"da7b354a-446e-49c8-9553-2613598a408e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting football/gfootball/scenarios/my_academy.py\n"]}]},{"cell_type":"code","source":["%%bash\n","cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -qq . && cd ..\n","rm -rf football kaggle-environments"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"746TiWVEkubi","executionInfo":{"status":"ok","timestamp":1652112434827,"user_tz":-180,"elapsed":13979,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"149feb53-ff65-4c85-896b-a5af32c4af50"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"]}]},{"cell_type":"code","source":["env = football_env.create_environment(env_name ='my_academy',render=False,representation='simple115v2')  #List with the 115 states \n","observation = env.reset()\n","new_observation,reward,done,info = env.step(5)\n","new_observation,reward,done,info = env.step(7)\n","new_observation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlDu1w4HdfMx","executionInfo":{"status":"ok","timestamp":1652112459534,"user_tz":-180,"elapsed":7,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"1dc0617f-a455-4a58-f702-24ab370b89ea"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Inside scenario build\n","Inside scenario build\n","Inside scenario build\n","Inside scenario build\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-1.01083136e+00,  1.62926114e-11,  7.00202398e-03, -6.90468587e-04,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00,  1.31901575e-03,  1.07590589e-10,\n","        5.18680830e-03, -1.21849218e-04, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","        1.00818884e+00, -2.04334587e-11, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -4.04610345e-03, -2.45929405e-11,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n","        1.99999996e-02, -0.00000000e+00,  1.09301046e-01,  0.00000000e+00,\n","       -0.00000000e+00, -1.21397059e-02,  1.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00,  0.00000000e+00,  0.00000000e+00], dtype=float32)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"H3BE01vX32gO","executionInfo":{"status":"ok","timestamp":1652111647967,"user_tz":-180,"elapsed":30,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["class DeepQNetwork(nn.Module):\n","  def __init__(self,lr,input_dims,fc1_dims,fc2_dims,fc3_dims,n_actions):\n","    super(DeepQNetwork,self).__init__()\n","   # self.lr=lr\n","    self.input_dims=input_dims\n","    self.fc1_dims=fc1_dims\n","    self.fc2_dims=fc2_dims\n","    self.fc3_dims=fc3_dims\n","    self.n_actions=n_actions\n","    \n","    self.fc1=nn.Linear(*self.input_dims,self.fc1_dims) #pass list of observations as input\n","    self.fc2=nn.Linear(self.fc1_dims,self.fc2_dims)\n","    self.fc3=nn.Linear(self.fc2_dims,self.fc3_dims)\n","    self.fc3=nn.Linear(self.fc3_dims,self.n_actions) #output number action\n","\n","    self.optimizer = optim.Adam(self.parameters(),lr=lr)\n","    self.loss=nn.MSELoss()\n","    self.device =T.device('cuda:0' if T.cuda.is_available() else 'cpu' )\n","    self.to(self.device)\n","\n","  def forward(self,state):\n","   \n","    x=F.relu(self.fc1(state))\n","    x=F.relu(self.fc2(x))\n","    actions=self.fc3(x)\n","      \n","    return actions\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XjBwyiw_32zj","executionInfo":{"status":"ok","timestamp":1652111647968,"user_tz":-180,"elapsed":28,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["class All_prints():\n","  \n","  def __init___(self,step):\n","    self.step=step\n","    #self.RewBuffer = RewBuffer\n","    self.reward=reward\n","\n","  def printstats(self,step,rew_list,eps_reward,epsilon):  #Kaleitai otan ginei done , diladi otan teleiosei ena paixnidi\n","    self.step=step\n","    self.rew_list=rew_list\n","    self.eps_reward=eps_reward\n","    self.epsilon=epsilon\n","    print(\"-->Episode:\",i%3000 + 1,\"\\t\",\"Episode Reward:\",eps_reward,\"\\t Epsilon\",agent.epsilon,\"<--\")\n","    #print(\"Step\",step)\n","    print(\"lista apo rewards mexri tora\" ,self.rew_list)\n","    print(\"Avg reward\", np.mean(self.rew_list))\n","    print(\"---------------------------------------------------\")\n","\n","  def print_who_scored(self, reward):\n","    self.reward=reward\n","    if(self.reward==1):\n","      print(\"our team scored !!!\")\n","    elif(self.reward ==-1):\n","      print(\"opponent team scored\")\n","    \n","    \n"," \n","  def rew_graph(self,rew_list,num_of_eps):\n","      self.rew_list=rew_list\n","      \n","      self.num_of_eps=num_of_eps\n","      \n","      eps_list=list(range(1,self.num_of_eps+1))#pairnei to proto , den pairnei to teleytaio\n","      \n","      plt.plot(eps_list,self.rew_list)\n","      plt.xlabel('Episode')\n","      plt.ylabel('Rewards')\n","      plt.grid(True)\n","      plt.show()\n","\n","\n","  def score_graph(self,score_list,num_of_eps):\n","      self.score_list=score_list\n","      \n","      self.num_of_eps=num_of_eps\n","      \n","      eps_list=list(range(1,self.num_of_eps+1))#pairnei to proto , den pairnei to teleytaio\n","      \n","      plt.plot(eps_list,self.score_list)\n","      plt.xlabel('Episode')\n","      plt.ylabel('Score')\n","      plt.grid(True)\n","      plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TDmo03iN32qO","executionInfo":{"status":"ok","timestamp":1652111647968,"user_tz":-180,"elapsed":28,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["class Agent():\n","  def __init__(self,gamma,epsilon, lr , input_dims , batch_size ,n_actions, max_mem_size = 40000  , eps_end=0.01 , eps_dec = 5e-4):\n","    self.gamma=gamma\n","    self.epsilon =epsilon\n","    self.lr=lr\n","    self.eps_min=eps_end\n","    self.eps_dec=eps_dec\n","   \n","    self.action_space =[i for i in range(n_actions)]\n","    self.mem_size = max_mem_size\n","    self.batch_size = batch_size\n","    self.n_actions=n_actions\n","    self.mem_cntr =0 # keep track of the position of first available memory \n","\n","    self.Q_eval = DeepQNetwork(self.lr,n_actions=n_actions,input_dims= input_dims, fc1_dims=600, fc2_dims=600 , fc3_dims=600)\n","\n","    self.state_memory = np.zeros((self.mem_size,*input_dims),dtype =np.float32)\n","    self.new_state_memory= np.zeros((self.mem_size , *input_dims),dtype=np.float32)\n","\n","    self.action_memory=np.zeros(self.mem_size , dtype=np.int32) #discrete actions (19)\n","    self.reward_memory=np.zeros(self.mem_size,dtype= np.float32)\n","    self.terminal_memory= np.zeros(self.mem_size,dtype=bool)\n","\n","\n","  def store_transition(self,state,action,reward,new_state , done ):\n","    index = self.mem_cntr% self.mem_size\n","\n","    self.state_memory[index]= state\n","    self.new_state_memory[index]= new_state\n","    self.reward_memory[index]= reward\n","    self.action_memory[index]= action  #which action is taken \n","    self.terminal_memory[index]= done\n","\n","    self.mem_cntr +=1\n","    \n","\n","  def choose_action(self,observation):\n","    if np.random.random()> self.epsilon:\n","      \n","      state =T.tensor([observation]).to(self.Q_eval.device) #turn observation to tensor and send it to device for computations\n","      action_list = self.Q_eval.forward(state) #returns the values of each action\n","      action = T.argmax(action_list).item()\n","      #print(\"exploit:\",action)\n","    else:     #\n","      \n","      action = np.random.choice(self.action_space)\n","      #print(\"explore:\",action)\n","    return action\n","\n","  def learn(self):    #fill batch size then learn \n","    if self.mem_cntr < self.batch_size :\n","      return\n","    \n","    \n","    self.Q_eval.optimizer.zero_grad()\n","\n","    #calculate the position of max memory / extract subset of max memories\n","    max_mem =min(self.mem_cntr , self.mem_size)\n","    for i in range(self.mem_size//self.batch_size):\n","      #batch=np.random.choice(max_mem,self.batch_size,replace=False) #We dont keep selecting the same memories more than once\n","     \n","      batch = np.random.permutation(max_mem)[:self.batch_size]\n","      #mem = np.array(exp_buffer)[perm_batch]\n","\n","      #batch=np.random.choice(max_mem,self.batch_size,replace=False)\n","      batch_index = np.arange(self.batch_size , dtype=np.int32)\n","\n","      state_batch=T.tensor(self.state_memory[batch]).to(self.Q_eval.device) #make numpy array a pytorch tensor\n","      new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n","      reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n","      terminal_batch= T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n","\n","      action_batch = self.action_memory[batch] \n","    \n","      q_eval = self.Q_eval.forward(state_batch)[batch_index,action_batch] #EXEI THEMA\n","      q_next = self.Q_eval.forward(new_state_batch)\n","\n","      q_next[terminal_batch] = 0.0\n","      q_target = reward_batch +self.gamma * T.max(q_next,dim=1)[0] #max value of next state\n","\n","      loss = self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n","      loss.backward()\n","      self.Q_eval.optimizer.step()\n","\n","      self.epsilon = self.epsilon - self.eps_dec if (self.epsilon > self.eps_min)  else self.eps_min\n","      "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"lVDb0TfR4zWR","executionInfo":{"status":"error","timestamp":1652111670231,"user_tz":-180,"elapsed":22290,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"8a335989-d40e-411a-b549-239e346f0cf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["shout entos periohis episode 0 step= 101\n","ball is out -10 episode 0 step= 113\n","---Avg reward last: -23.217390593432082 Avg score last 0.0 Avg steps 114.0 episode= 0 ---\n","shout entos periohis episode 1 step= 104\n","shout entos periohis episode 1 step= 106\n","goal episode 1 step= 115\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-76342f9ee157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-1d4c4050e920>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#TODO: NA BGALO TON PAIKTI\n","\n","#if __name__ == '__main__':\n","#env = football_env.create_environment(env_name ='11_vs_11_easy_stochastic',render=False,representation='simple115')  #List with the 115 states \n","env = football_env.create_environment(env_name ='academy_empty_goal',render=False,representation='simple115v2')  #List with the 115 states \n","#env = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\") #List with the 115 states\n","#env = football_env.create_environment(env_name ='academy_single_goal_versus_lazy',render=False,representation='simple115')\n","#env = football_env.create_environment(env_name ='academy_pass_and_shoot_with_keeper',render=False,representation='simple115')\n","#env = football_env.create_environment(env_name ='academy_empty_goal',render=False,representation='simple115')  #List with the 115 states \n","\n","#CUSTOMIZE ACTION LIST AND OBSERVATIONS\n","Action_list=[4,5,6,12]\n","\n","#Create Objects\n","\n","agent = Agent(gamma=0.99,epsilon=1.0 ,batch_size = 264 ,lr=0.00015 ,input_dims= [115], n_actions = len(Action_list) )# batch = best 256\n","all_prints = All_prints()\n","#cus_rew =Custom_Rewards()\n","\n","scores,ep_history =[],[]\n","\n","steps=0\n","terminal =0\n","episode =0\n","\n","\n","\n","num_of_eps = 1000\n","eps_rew=0\n","rew_list =[]\n","score_list = []\n","step_list =[]\n","for i in range(num_of_eps) : \n","  score =0 \n","  done=False \n","  observation =env.reset()\n","  act =0 #first action will be to move right \n","  while not done:\n","\n","    #CUSTOMIZE ACTIONS HERE \n","    #An einai i mpala sto 0.5 kai exo katoxi kane shout diladi action 12\n","    \n","    #print(\"------------\")\n","    #print(\"Ball X-Y-Z Axis\",observation[88],observation[89],observation[90],\"||\",\"direct\",observation[91],observation[92],observation[93],\"Katoxi\",observation[95],observation[96])\n","    #print(\"Player X-Axis Y-Axis\",observation[2],observation[3] ,\"episode\",i)\n","    #print(\"------------\")\n","\n","    #na bazei goal aytomata\n","    #if(observation[2]>0.6 and observation[95] == 1):\n","      \n","      \n","      #print(\"EXO MPALA\",12)\n","      #new_observation,reward,done,info = env.step(12) #kanei shout Action 12\n","\n","      #print(\"ball position\",observation[88], \"Action taken\", action)\n","     \n","      \n"," \n","    if(act ==0 ):\n","      \n","      # print(\"action 5\",Action_list[action])\n","      new_observation,reward,done,info = env.step(5)\n","      act=1\n","    #print(\"Sto Else\",observation[94],observation[95],observation[96])\n","    \n","\n","    \n","    action = agent.choose_action(observation)\n","    while((observation[2]<0.65)  and (Action_list[action]==12)):\n","      action = agent.choose_action(observation)\n","\n","    new_observation,reward,done,info = env.step(Action_list[action])\n","\n","    #if(reward!=1):  #each step loses -0.2\n","      #reward= reward -0.2\n","    \n","    \n","\n","    if(done ==1 and reward !=1): #if ball is out ,loses -2\n","      reward = reward -10\n","      print(\"ball is out -10\",\"episode\",i,\"step=\",steps)\n","    if(done ==1 and reward !=1 and observation[96]==1):\n","      print(\"STEAL\")\n","    if(reward==1 and  done ==1): #if agent scores , wins +5\n","      print(\"goal\",\"episode\",i,\"step=\",steps)\n","      reward = 10\n","\n","\n","    if((observation[2]<0.5) and (observation[95] == 1) and (Action_list[action]==12)): #an shoutarei prin th megali perioxh -2\n","      \n","      reward= reward -200\n","      done=1\n","      print(\"shout ektos periohis\",\"episode\",i,\"step=\",steps)\n","\n","    if((observation[2]>0.5) and (observation[95] == 1) and (Action_list[action]==12)): #an shoutarei mesa ti megali periohi +0.1\n","      #reward= reward +0.1\n","      print(\"shout entos periohis\",\"episode\",i,\"step=\",steps)\n","      \n","    reward = reward - ( math.sqrt( ((0.935 - observation[2])**2) + (0 -observation[3])**2 ) *0.2) #oso pio makria einai toso perissotero xanei\n","    #print(\"Den exo mpala\",Action_list[action])\n","    #print(\"DEN EXO MPALA\",Action_list[action],action)\n","\n","    #custom reward here\n"," \n","    \n","    #if( ((observation[88]>0.99) and (observation[95]==1)) or  ((observation[89]>0.39) and (observation[95]==1)) or ((observation[89] < -0.39)and(observation[95]==1)) ):\n","      #reward =-2\n","      #print(\"bgike ektos\")\n","    #if((reward!=1) or (reward!=-1)):\n","      #reward =cus_rew.custom_rew(observation,action,reward,new_observation)\n","    score= +reward\n","\n","    #for prints\n","    eps_rew+=reward\n","    #all_prints.print_who_scored(reward)\n","    \n","\n","    agent.store_transition(observation,action,reward,new_observation,done)\n","    agent.learn()\n","    observation = new_observation\n","\n","    scores.append(score)\n","    ep_history.append(agent.epsilon)\n","\n","    avg_score= np.mean(scores)\n","\n","\n","    steps=steps+1\n","    \n","#---- BE CAREFUL OF THE WHILE !!! HERE IS EPIDOSE ENDING--------\n","  #print(\"Reward\",eps_rew,\"Episode\",i,\"Steps\" , steps)\n","  step_list.append(steps)\n","  steps=0\n","\n","  val = info.values()\n","  list_val=list(val)\n","  score_list.append(list_val)\n","  \n","  #terminate if 500 episodes are correct \n","  if(eps_rew == 1 ):\n","    terminal= terminal +1\n","    if(terminal == 500):\n","      print(\"500 Episodes with goal\")\n","      break\n","  else:\n","      terminal =0 \n","  episode = episode +1 \n","  \n","\n","  rew_list.append(eps_rew)\n","\n","# PRINTS\n","  if (i % 10)== 0 :\n","      print(\"---Avg reward last:\", np.mean(rew_list[-10:]),\"Avg score last\",np.mean(score_list[-10:]),\"Avg steps\",np.mean(step_list[-10:]),\"episode=\",i,\"---\")\n","      #print(score_list)\n","\n","  if (((i % 1000)== 0) and i!=0) :\n","      all_prints.score_graph(score_list[-1000:],1000)# graph the last 1000 episodes\n","  if(((i%50)==0) and i!=0):\n","    all_prints.rew_graph(rew_list[-i:],i)\n","     \n","  \n","  #EPISODE PRINTS\n","  #all_prints.printstats(i,rew_list,eps_rew,agent.epsilon)\n","  \n","  eps_rew=0\n","\n","\n","\n","print(\"Avg score last:\", np.mean(rew_list[-10:]),\"Avg score\",np.mean(score_list),\"Avg steps\",np.mean(step_list[-10:]),\"episode=\",i)\n","all_prints.score_graph(score_list[-1000:],1000)# graph the last 1000 episodes\n","      \n","#all_prints.score_graph(score_list,i+1)#i = num_of_eps\n","#print(\"\\n \")\n","#all_prints.rew_graph(rew_list,i+1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHOOJstUDEPX","executionInfo":{"status":"aborted","timestamp":1652111670213,"user_tz":-180,"elapsed":22,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["#plot step_list plt.plot(step_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW6cEVwxTpKB","executionInfo":{"status":"aborted","timestamp":1652111670220,"user_tz":-180,"elapsed":28,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["class Custom_Rewards():\n","  #def __init__(self):\n","    #self.obs=obs    \n","    #self.action=action\n","    #self.reward =reward\n","  def custom_rew(self,obs,action,reward,new_obs):\n","    \n","    self.obs=obs\n","    self.action = action\n","    self.reward = reward\n","    self.new_obs=new_obs\n","    \n","    \n","#if the next action is shooting the ball, the reward will gain 0.2 point ||\n","#an exo ti mpala kai einai se apostasi x > 0.5 sto gipedo kai exei kanei shout einai kalo    \n","    if(self.obs[95] == 1 and self.obs[88] > 0.6 and self.action == 12):\n","      self.reward += 0.02\n","      #print(\"EKANE SHOUT MPROSTA APO TO KENTRO ,+0.1\")\n","      return self.reward\n","\n","    #An kanei shout piso apo to kentro na xanei ligo\n","    elif(self.obs[95]==1 and self.obs[88] < 0 and self.action == 12):\n","      self.reward -= 0.02\n","      #print(\"EKANE SHOUT PISO APO TO KENTRO, -0.05\")\n","      return self.reward\n","\n","# if we steal the ball, the reward will gain 0.05 points\n","    elif self.obs[96] == 1 and self.new_obs[95] == 1:  #obs[96] ball owner right team / obs[95] ball owner left team\n","        self.reward += 0.05\n","        #print(\"PHRE THN MPALA APO TON ANTIPALO , +0.05\")\n","        return self.reward\n","#if we lose the ball, the reward will lose 0.05 points\n","    elif self.obs[95] == 1 and self.new_obs[96] == 1:\n","        self.reward -=0.02\n","        #print(\"PHRE THN MPALA O ANTIPALOS , -0.02\")\n","        return self.reward\n","    else:\n","      return self.reward\n","\n","\n","#TODO: if active player far from the ball, the reward will lose by the moving distance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuWQL9XXhuGk","executionInfo":{"status":"aborted","timestamp":1652111670222,"user_tz":-180,"elapsed":30,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["#print(score_list or reward_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1652111670223,"user":{"displayName":"george mouts","userId":"12301814581979843830"},"user_tz":-180},"id":"SDWw5Cs_VQpG"},"outputs":[],"source":["\n","\"\"\"print(observation)\n","new_observation,reward,done,info = env.step(4)\n","print(new_observation)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68UgCfWlVQ3A","executionInfo":{"status":"aborted","timestamp":1652111670224,"user_tz":-180,"elapsed":32,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1u0L3hS4zlT","executionInfo":{"status":"aborted","timestamp":1652111670225,"user_tz":-180,"elapsed":32,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD_RP0kIhyeX","executionInfo":{"status":"aborted","timestamp":1652111670227,"user_tz":-180,"elapsed":34,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":["\"\"\"env = football_env.create_environment(env_name ='academy_single_goal_versus_lazy',render=False,representation='simple115')\n","observation =env.reset()\n","new_observation,reward,done,info = env.step(action)\n","val=info.values()\n","print(val)\n","list_val=list(val)\n","list_val\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bK6k0DgGtRu4","executionInfo":{"status":"aborted","timestamp":1652111670229,"user_tz":-180,"elapsed":36,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"4_actions.ipynb","provenance":[],"authorship_tag":"ABX9TyNGCQNl5dypT0Mo60J2iX+R"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}