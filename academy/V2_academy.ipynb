{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"V2_academy.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOBx9PslGwXHfBXoetVroQ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzafyTmK3yQQ","executionInfo":{"status":"ok","timestamp":1649947419301,"user_tz":-180,"elapsed":33778,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"bf92331f-8334-4f27-d0e3-b1c246517892"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'kaggle-environments' already exists and is not an empty directory.\n","Processing /content/kaggle-environments\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.9) (4.3.3)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.9) (1.1.4)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.9) (1.21.5)\n","Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.9) (2.27.1)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.9) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.9) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.9) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.9) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.1.2->kaggle-environments==1.9.9) (2.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.9) (4.1.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.9) (5.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.9) (4.11.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.9) (0.18.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.9) (21.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kaggle-environments==1.9.9) (3.8.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.9) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.9) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.9) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.9) (2.0.12)\n","Building wheels for collected packages: kaggle-environments\n","  Building wheel for kaggle-environments (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle-environments: filename=kaggle_environments-1.9.9-py3-none-any.whl size=1815529 sha256=51c9a67070eb501998c89bbb874141bc0687efae73c9ef6433aae31429590e71\n","  Stored in directory: /root/.cache/pip/wheels/67/f1/54/59176bd30840c0a045df67632e2e903095b3c02b64cb0a636c\n","Successfully built kaggle-environments\n","Installing collected packages: kaggle-environments\n","  Attempting uninstall: kaggle-environments\n","    Found existing installation: kaggle-environments 1.9.9\n","    Uninstalling kaggle-environments-1.9.9:\n","      Successfully uninstalled kaggle-environments-1.9.9\n","Successfully installed kaggle-environments-1.9.9\n","Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 252 kB in 2s (123 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libsdl2-gfx-dev is already the newest version (1.0.4+dfsg-1).\n","libsdl2-ttf-dev is already the newest version (2.0.14+dfsg1-2).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","fatal: destination path 'football' already exists and is not an empty directory.\n","--2022-04-14 14:43:16--  https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 172.217.203.128, 108.177.13.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 45403632 (43M) [application/octet-stream]\n","Saving to: ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’\n","\n","football/third_part 100%[===================>]  43.30M   173MB/s    in 0.2s    \n","\n","2022-04-14 14:43:17 (173 MB/s) - ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’ saved [45403632/45403632]\n","\n","Processing /content/football\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: pygame==1.9.6 in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.9.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.4.1)\n","Requirement already satisfied: gym>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.17.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.37.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.21.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.11.0->gfootball==2.8) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->gfootball==2.8) (1.15.0)\n","Building wheels for collected packages: gfootball\n","  Building wheel for gfootball (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gfootball: filename=gfootball-2.8-cp37-cp37m-linux_x86_64.whl size=38781744 sha256=8062af838c05d037901286139f1ed958ec659afdc8c9928367e3326ca8a02fce\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0dvqeoo5/wheels/bb/c2/92/82a23d0c207f5497a23f4316675eb1629e6be474bd2c3be61a\n","Successfully built gfootball\n","Installing collected packages: gfootball\n","  Attempting uninstall: gfootball\n","    Found existing installation: gfootball 2.8\n","    Uninstalling gfootball-2.8:\n","      Successfully uninstalled gfootball-2.8\n","Successfully installed gfootball-2.8\n","Requirement already satisfied: utils in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"]}],"source":["#https://www.kaggle.com/piotrstanczyk/gfootball-template-bot  G-FOOTBALL TEMPLATE BOT\n","# Install:\n","# Kaggle environments.\n","!git clone https://github.com/Kaggle/kaggle-environments.git\n","!cd kaggle-environments && pip install .\n","\n","# GFootball environment.\n","!apt-get update -y\n","!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n","\n","# Make sure that the Branch in git clone and in wget call matches !!\n","!git clone -b v2.8 https://github.com/google-research/football.git\n","!mkdir -p football/third_party/gfootball_engine/lib\n","\n","!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n","!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n","\n","!pip install utils\n","#!pip install learning"]},{"cell_type":"code","source":["#from gfootball.env.football_env import FootballEnv\n","from kaggle_environments import make\n","from gfootball.env.config import Config\n","import gfootball.env as football_env\n","\n","#import dqn libraries\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim \n","import numpy as np\n","import pandas as pd\n","import itertools\n","import utils\n","import random\n","from collections import deque\n","import matplotlib.pyplot as plt\n","#from utils import plot_learning_curve\n","#import env \n","import gym\n","import gfootball \n","\n","#env_name = \"GFootballBase-v0\"\n","#print(env_name)"],"metadata":{"id":"b_qvwQbE32B_","executionInfo":{"status":"ok","timestamp":1649947512826,"user_tz":-180,"elapsed":310,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class DeepQNetwork(nn.Module):\n","  def __init__(self,lr,input_dims,fc1_dims,fc2_dims,fc3_dims,n_actions):\n","    super(DeepQNetwork,self).__init__()\n","   # self.lr=lr\n","    self.input_dims=input_dims\n","    self.fc1_dims=fc1_dims\n","    self.fc2_dims=fc2_dims\n","    self.fc3_dims=fc3_dims\n","    self.n_actions=n_actions\n","    \n","    self.fc1=nn.Linear(*self.input_dims,self.fc1_dims) #pass list of observations as input\n","    self.fc2=nn.Linear(self.fc1_dims,self.fc2_dims)\n","    self.fc3=nn.Linear(self.fc2_dims,self.fc3_dims)\n","    self.fc3=nn.Linear(self.fc3_dims,self.n_actions) #output number action\n","\n","    self.optimizer = optim.Adam(self.parameters(),lr=lr)\n","    self.loss=nn.MSELoss()\n","    self.device =T.device('cuda:0' if T.cuda.is_available() else 'cpu' )\n","    self.to(self.device)\n","\n","  def forward(self,state):\n","   \n","    x=F.relu(self.fc1(state))\n","    x=F.relu(self.fc2(x))\n","    actions=self.fc3(x)\n","      \n","    return actions\n"],"metadata":{"id":"H3BE01vX32gO","executionInfo":{"status":"ok","timestamp":1649947419305,"user_tz":-180,"elapsed":48,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class Agent():\n","  def __init__(self,gamma,epsilon, lr , input_dims , batch_size ,n_actions, max_mem_size = 100000  , eps_end=0.01 , eps_dec = 5e-4):\n","    self.gamma=gamma\n","    self.epsilon =epsilon\n","    self.lr=lr\n","    self.eps_min=eps_end\n","    self.eps_dec=eps_dec\n","   \n","    self.action_space =[i for i in range(n_actions)]\n","    self.mem_size = max_mem_size\n","    self.batch_size = batch_size\n","    self.n_actions=n_actions\n","    self.mem_cntr =0 # keep track of the position of first available memory \n","\n","    self.Q_eval = DeepQNetwork(self.lr,n_actions=n_actions,input_dims= input_dims, fc1_dims=700, fc2_dims=700 , fc3_dims=700)\n","\n","    self.state_memory = np.zeros((self.mem_size,*input_dims),dtype =np.float32)\n","    self.new_state_memory= np.zeros((self.mem_size , *input_dims),dtype=np.float32)\n","\n","    self.action_memory=np.zeros(self.mem_size , dtype=np.int32) #discrete actions (19)\n","    self.reward_memory=np.zeros(self.mem_size,dtype= np.float32)\n","    self.terminal_memory= np.zeros(self.mem_size,dtype=np.bool)\n","\n","\n","  def store_transition(self,state,action,reward,new_state , done ):\n","    index = self.mem_cntr% self.mem_size\n","\n","    self.state_memory[index]= state\n","    self.new_state_memory[index]= new_state\n","    self.reward_memory[index]= reward\n","    self.action_memory[index]= action  #which action is taken \n","    self.terminal_memory[index]= done\n","\n","    self.mem_cntr +=1\n","    \n","\n","  def choose_action(self,observation):\n","    if np.random.random()> self.epsilon:\n","      state =T.tensor([observation]).to(self.Q_eval.device) #turn observation to tensor and send it to device for computations\n","      action_list = self.Q_eval.forward(state) #returns the values of each action\n","      action = T.argmax(action_list).item()\n","    else:     #\n","      action = np.random.choice(self.action_space)\n","\n","    return action\n","\n","  def learn(self):    #fill batch size then learn \n","    if self.mem_cntr < self.batch_size :\n","      return\n","    \n","\n","    self.Q_eval.optimizer.zero_grad()\n","\n","    #calculate the position of max memory / extract subset of max memories\n","    max_mem =min(self.mem_cntr , self.mem_size)\n","    batch=np.random.choice(max_mem,self.batch_size,replace=False) #We dont keep selecting the same memories more than once\n","\n","    batch_index = np.arange(self.batch_size , dtype=np.int32)\n","\n","    state_batch=T.tensor(self.state_memory[batch]).to(self.Q_eval.device) #make numpy array a pytorch tensor\n","    new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n","    reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n","    terminal_batch= T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n","\n","    action_batch = self.action_memory[batch] \n","    \n","    q_eval = self.Q_eval.forward(state_batch)[batch_index,action_batch] #EXEI THEMA\n","    q_next = self.Q_eval.forward(new_state_batch)\n","\n","    q_next[terminal_batch] = 0.0\n","    q_target = reward_batch +self.gamma * T.max(q_next,dim=1)[0] #max value of next state\n","\n","    loss = self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n","    loss.backward()\n","    self.Q_eval.optimizer.step()\n","\n","    self.epsilon = self.epsilon - self.eps_dec if (self.epsilon > self.eps_min)  else self.eps_min\n","     "],"metadata":{"id":"TDmo03iN32qO","executionInfo":{"status":"ok","timestamp":1649947419307,"user_tz":-180,"elapsed":49,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class All_prints():\n","  \n","  def __init___(self,step):\n","    self.step=step\n","    #self.RewBuffer = RewBuffer\n","    self.reward=reward\n","\n","  def printstats(self,step,rew_list,eps_reward,epsilon):  #Kaleitai otan ginei done , diladi otan teleiosei ena paixnidi\n","    self.step=step\n","    self.rew_list=rew_list\n","    self.eps_reward=eps_reward\n","    self.epsilon=epsilon\n","    print(\"-->Episode:\",i%3000 + 1,\"\\t\",\"Episode Reward:\",eps_reward,\"\\t Epsilon\",agent.epsilon,\"<--\")\n","    #print(\"Step\",step)\n","    print(\"lista apo rewards mexri tora\" ,self.rew_list)\n","    print(\"Avg reward\", np.mean(self.rew_list))\n","    print(\"---------------------------------------------------\")\n","\n","  def print_who_scored(self, reward):\n","    self.reward=reward\n","    if(self.reward==1):\n","      print(\"our team scored !!!\")\n","    elif(self.reward ==-1):\n","      print(\"opponent team scored\")\n","    \n","    \n"," \n","  def rew_graph(self,rew_list,num_of_eps):\n","      self.rew_list=rew_list\n","      \n","      self.num_of_eps=num_of_eps\n","      \n","      eps_list=list(range(1,self.num_of_eps+1))#pairnei to proto , den pairnei to teleytaio\n","      \n","      plt.plot(eps_list,self.rew_list)\n","      plt.xlabel('Episode')\n","      plt.ylabel('Rewards')\n","      plt.grid(True)\n","      plt.show()"],"metadata":{"id":"XjBwyiw_32zj","executionInfo":{"status":"ok","timestamp":1649947419308,"user_tz":-180,"elapsed":49,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class Custom_Rewards():\n","  #def __init__(self):\n","    #self.obs=obs    \n","    #self.action=action\n","    #self.reward =reward\n","  def custom_rew(self,obs,action,reward,new_obs):\n","    \n","    self.obs=obs\n","    self.action = action\n","    self.reward = reward\n","    self.new_obs=new_obs\n","    \n","    \n","#if the next action is shooting the ball, the reward will gain 0.2 point ||\n","#an exo ti mpala kai einai se apostasi x > 0.5 sto gipedo kai exei kanei shout einai kalo    \n","    if(self.obs[95] == 1 and self.obs[88] > 0.6 and self.action == 12):\n","      self.reward += 0.1\n","      #print(\"EKANE SHOUT MPROSTA APO TO KENTRO ,+0.1\")\n","      return self.reward\n","\n","    #An kanei shout piso apo to kentro na xanei ligo\n","    elif(self.obs[95]==1 and self.obs[88] < 0 and self.action == 12):\n","      self.reward -= 0.02\n","      #print(\"EKANE SHOUT PISO APO TO KENTRO, -0.05\")\n","      return self.reward\n","\n","# if we steal the ball, the reward will gain 0.05 points\n","    elif self.obs[96] == 1 and self.new_obs[95] == 1:  #obs[96] ball owner right team / obs[95] ball owner left team\n","        self.reward += 0.05\n","        #print(\"PHRE THN MPALA APO TON ANTIPALO , +0.05\")\n","        return self.reward\n","#if we lose the ball, the reward will lose 0.05 points\n","    elif self.obs[95] == 1 and self.new_obs[96] == 1:\n","        self.reward -=0.02\n","        #print(\"PHRE THN MPALA O ANTIPALOS , -0.02\")\n","        return self.reward\n","    else:\n","      return self.reward\n","\n","\n","#TODO: if active player far from the ball, the reward will lose by the moving distance"],"metadata":{"id":"eW6cEVwxTpKB","executionInfo":{"status":"ok","timestamp":1649949108522,"user_tz":-180,"elapsed":282,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["#TODO: DOKIMASE ENA ALLO SCENARIO NA DEIS AN EKPAIDEYETAI\n","#TODO: if active player far from the ball, the reward will lose by the moving distance\n","#TODO: FTIAKSE PIO POLY TA CUSTOM REWARDS\n","\n","#if __name__ == '__main__':\n","env = football_env.create_environment(env_name ='academy_empty_goal_close',render=False,representation='simple115')  #List with the 115 states \n","#env = football_env.create_environment(env_name ='academy_empty_goal_close',render=False,representation='simple115')  #List with the 115 states \n","#env = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\") #List with the 115 states\n","\n","#Create Objects\n","agent = Agent(gamma=0.99,epsilon=1.0 ,batch_size =64,lr=0.003 ,input_dims= [115], n_actions = env.action_space.n )\n","all_prints = All_prints()\n","cus_rew =Custom_Rewards()\n","\n","scores,ep_history =[],[]\n","\n","num_of_eps = 1500\n","eps_rew=0\n","rew_list =[]\n","for i in range(num_of_eps) : #edo mesa na mpei number of ep , etsi k allios bgainei me to done \n","  score =0 \n","  done=False \n","  observation =env.reset()\n","  while not done:\n","    action = agent.choose_action(observation)\n","\n","    new_observation,reward,done,info = env.step(action)\n","    #custom reward here\n","    \n","    if((reward!=1) or (reward!=-1)):\n","      reward =cus_rew.custom_rew(observation,action,reward,new_observation)\n","    score= +reward\n","\n","    #for prints\n","    eps_rew+=reward\n","    all_prints.print_who_scored(reward)\n","    \n","\n","    agent.store_transition(observation,action,reward,new_observation,done)\n","    agent.learn()\n","    observation = new_observation\n","\n","    scores.append(score)\n","    ep_history.append(agent.epsilon)\n","\n","    avg_score= np.mean(scores)\n","\n","#---- BE CAREFUL OF THE WHILE !!! HERE IS EPIDOSE ENDING\n","  \n","  rew_list.append(eps_rew)\n","\n","  #EPISODE PRINTS\n","  all_prints.printstats(i,rew_list,eps_rew,agent.epsilon)\n","  \n","  eps_rew=0\n","  \n","all_prints.rew_graph(rew_list,num_of_eps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1yjqknTz7Kx7kJD9I55qsida7Jrno5PI1"},"id":"lVDb0TfR4zWR","executionInfo":{"status":"ok","timestamp":1649948963047,"user_tz":-180,"elapsed":1315377,"user":{"displayName":"george mouts","userId":"12301814581979843830"}},"outputId":"dffb5ff5-df02-4afb-ae38-4c21a43284f7"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)"],"metadata":{"id":"P1u0L3hS4zlT","executionInfo":{"status":"aborted","timestamp":1649947419643,"user_tz":-180,"elapsed":376,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MD_RP0kIhyeX","executionInfo":{"status":"aborted","timestamp":1649947419645,"user_tz":-180,"elapsed":5,"user":{"displayName":"george mouts","userId":"12301814581979843830"}}},"execution_count":null,"outputs":[]}]}